{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Курсовая работа за 4 семестр\n",
    "## Островерхов Артем, КН-202\n",
    "___\n",
    "# House Prices - Advanced Regression Techniques\n",
    "### Формулировка задачи:\n",
    "Сравнение использования различных архитектур нейронных сетей для решения задач регрессии, на примере реальной задачи с Kaggle\\\n",
    "https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/overview\n",
    "\n",
    "**Цель данного проекта** — построить модель машинного обучения для прогнозирования конечной цены продажи жилых домов в городе Эймс, штат Айова. Предоставленный набор данных включает 79 признаков, описывающих различные аспекты недвижимости: от площади и количества комнат до качества отделки и расположения. Эта задача относится к типу регрессии, где необходимо предсказать непрерывную числовую переменную — цену продажи дома.\n",
    "___\n",
    "# Описание задачи\n",
    "Для начала посмотрим на данную мне задачу и разберем, что такое регрессия и что надо знать, прежде чем начать работу.\n",
    "\n",
    "**Регрессия** — это тип задачи машинного обучения, в которой модель учится предсказывать числовое значение на основе входных данных.\n",
    "\n",
    "___\n",
    "**Формальное определение**\n",
    "В задаче регрессии требуется найти такую функцию:\n",
    "$$\n",
    "f : \\mathbb{R}^n \\to \\mathbb{R}\n",
    "$$\n",
    "которая по входному вектору признаков:\n",
    "$$\n",
    "\\mathbf{x} = (x_1, x_2, \\dots, x_n)\n",
    "$$\n",
    "предсказывает выходное числовое значение:\n",
    "$$\n",
    "\\hat{y} = f(\\mathbf{x})\n",
    "$$\n",
    "таким образом, чтобы приближать истинное значение:\n",
    "$$\n",
    "y \\in \\mathbb{R}\n",
    "$$\n",
    "как можно точнее.\n",
    "___\n",
    "То есть можно переформулировать **цель обучения**\n",
    "Найти такую функцию $f(\\mathbf{x})$, которая... минимизирует ошибку предсказания.\n",
    "Например, для линейной регрессии мы ищем:\n",
    "$$\n",
    "\\hat{y} = w_0 + w_1 x_1 + w_2 x_2 + \\dots + w_n x_n = \\mathbf{w}^\\top \\mathbf{x} + b\n",
    "$$\n",
    "Модель обучается путём минимизации функции потерь — например, среднеквадратичной ошибки (MSE):\n",
    "$$\n",
    "\\mathrm{MSE} = \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{y}_i - y_i)^2\n",
    "$$\n",
    "где:\n",
    "- $m$ — количество примеров в обучающей выборке\n",
    "- $\\hat{y}_i$ — предсказание модели\n",
    "- $y_i$ — истинное значение\n",
    "___\n",
    "**Особенности набора данных**\n",
    "\n",
    "Набор данных содержит 1460 объектов в обучающей выборке и 1459 объектов в тестовой. Каждый дом описан 79 признаками, среди которых есть как числовые (GrLivArea, LotArea, YearBuilt), так и категориальные (Neighborhood, HouseStyle, Exterior1st и др.).\n",
    "\n",
    "Перед началом обучения важно:\\\n",
    "\t•\tобработать пропущенные значения;\\\n",
    "\t•\tзакодировать категориальные признаки (например, через one-hot или label encoding);\\\n",
    "\t•\tмасштабировать числовые признаки (например, с помощью StandardScaler);\\\n",
    "\t•\tпреобразовать целевую переменную логарифмически (для уменьшения влияния выбросов):\\\n",
    "$$\n",
    "y = \\log(\\text{SalePrice})\n",
    "$$\n",
    "___\n",
    "**Метрика оценки**\n",
    "\n",
    "На Kaggle используется метрика RMSLE (Root Mean Squared Logarithmic Error), которая вычисляется по формуле:\n",
    "\n",
    "$$\n",
    "\\text{RMSLE} = \\sqrt{ \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\log(\\hat{y}_i + 1) - \\log(y_i + 1) \\right)^2 }\n",
    "$$\n",
    "\n",
    "Данная метрика особенно полезна, когда важно уменьшить влияние больших ошибок при переоценке стоимости.\n",
    "Она делает больший акцент на относительные отклонения, а не на абсолютные, и более устойчива к выбросам.\n",
    "___\n",
    "# Ход работы\n",
    "\n",
    "**Цель данного этапа** — реализовать и сравнить несколько архитектур моделей регрессии, включая как классические, так и нейросетевые подходы.\n",
    "\n",
    "В рамках эксперимента были реализованы и обучены следующие модели:\n",
    "\n",
    "- **ElasticNet** — базовая линейная модель с регуляризацией\n",
    "- **MLP (Multi-Layer Perceptron)** — полносвязная нейронная сеть\n",
    "- **CNN1D (одномерная сверточная сеть)** — экспериментальное применение сверточной архитектуры к табличным данным\n",
    "\n",
    "Для каждой архитектуры проводились:\n",
    "\n",
    "1. Предобработка данных (кодирование, масштабирование, логарифмирование целевой переменной)\n",
    "2. Обучение модели с фиксированными параметрами\n",
    "3. Валидация качества предсказания по метрике RMSE\n",
    "4. Визуальный анализ ошибок\n",
    "\n",
    "Ниже приведено поэтапное описание подготовки данных и реализации каждой модели.\n",
    "___\n",
    "## 1. ElasticNet\n",
    "\n",
    "Модель ElasticNet применяется в задачах линейной регрессии, когда наблюдаются следующие проблемы:\n",
    "\n",
    "- Мультиколлинеарность признаков (сильная корреляция между переменными),\n",
    "- Наличие большого количества нерелевантных или шумовых признаков,\n",
    "- Необходимость интерпретируемости и устойчивости модели.\n",
    "\n",
    "ElasticNet представляет собой обобщение двух видов регуляризации:\n",
    "\n",
    "- L1-регуляризация (Lasso), зануляющая коэффициенты и производящая отбор признаков,\n",
    "- L2-регуляризация (Ridge), снижающая чувствительность модели к коррелированным признакам.\n",
    "\n",
    "### Математическая формулировка\n",
    "\n",
    "Целевая функция ElasticNet объединяет обе регуляризации и имеет следующий вид:\n",
    "\n",
    "$$\n",
    "\\min_{\\mathbf{w}, b} \\left\\{ \\frac{1}{2m} \\sum_{i=1}^{m} \\left( y_i - \\mathbf{w}^\\top \\mathbf{x}_i - b \\right)^2 + \\alpha \\left( \\rho \\|\\mathbf{w}\\|_1 + \\frac{1 - \\rho}{2} \\|\\mathbf{w}\\|_2^2 \\right) \\right\\}\n",
    "$$\n",
    "\n",
    "где:\n",
    "- $m$ — количество объектов обучающей выборки\n",
    "- $\\mathbf{x}_i \\in \\mathbb{R}^n$ — вектор признаков для $i$-го объекта\n",
    "- $y_i \\in \\mathbb{R}$ — соответствующее целевое значение\n",
    "- $\\mathbf{w} \\in \\mathbb{R}^n$, $b \\in \\mathbb{R}$ — параметры модели\n",
    "- $\\alpha > 0$ — коэффициент регуляризации\n",
    "- $\\rho \\in [0, 1]$ — параметр смешивания регуляризаторов\n",
    "- $\\|\\mathbf{w}\\|_1$ — сумма абсолютных значений коэффициентов (L1-норма)\n",
    "- $\\|\\mathbf{w}\\|_2^2$ — сумма квадратов коэффициентов (L2-норма)\n",
    "\n",
    "При $ \\rho = 1 $ модель эквивалентна Lasso, при $ \\rho = 0 $ — Ridge.\n",
    "Значения $ \\rho \\in (0, 1) $ обеспечивают компромисс между разреженностью и устойчивостью.\n",
    "\n",
    "### Подготовка данных\n",
    "\n",
    "Перед обучением модели были выполнены следующие шаги:\n",
    "\n",
    "- Логарифмическое преобразование целевой переменной:\n",
    "\n",
    "  $$\n",
    "  y = \\log(\\text{SalePrice} + 1)\n",
    "  $$\n",
    "\n",
    "- Объединение обучающей и тестовой выборок для единой предобработки,\n",
    "- Логарифмирование числовых признаков с высокой скошенностью:\n",
    "\n",
    "  $$\n",
    "  x' = \\log(x + 1), \\quad \\text{если} \\quad |\\text{skew}(x)| > 0.75\n",
    "  $$\n",
    "\n",
    "- Заполнение пропущенных значений:\n",
    "  - медианой — для числовых признаков,\n",
    "  - наиболее частой категорией — для категориальных.\n",
    "\n",
    "- Кодирование категориальных признаков методом One-Hot Encoding,\n",
    "- Масштабирование всех числовых признаков с помощью стандартизации (StandardScaler).\n",
    "\n",
    "### Гиперпараметры и обучение\n",
    "\n",
    "Модель обучалась с помощью метода координатного спуска с ограничением по числу итераций\n",
    "($ \\text{max\\_iter} = 5000 $) и критерием сходимости ($ \\text{tol} = 10^{-3} $).\n",
    "\n",
    "Подбор гиперпараметров ($ \\alpha $) и ($ \\rho $) производился с использованием GridSearchCV по сетке:\n",
    "\n",
    "- $ \\alpha \\in \\{10^{-3}, 10^{-2}, 10^{-1}, 1, 10, 10^2, 10^3\\} $\n",
    "- $ \\rho \\in \\{0.1, 0.5, 0.9\\} $\n",
    "\n",
    "Оптимизация осуществлялась по метрике RMSLE (Root Mean Squared Logarithmic Error):\n",
    "\n",
    "$$\n",
    "\\text{RMSLE} = \\sqrt{ \\frac{1}{n} \\sum_{i=1}^{n} \\left( \\log(\\hat{y}_i + 1) - \\log(y_i + 1) \\right)^2 }\n",
    "$$\n",
    "\n",
    "Такое логарифмическое преобразование целевой переменной снижает влияние выбросов и делает метрику более устойчивой к большим значениям."
   ],
   "id": "f1329658da10d979"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def rmsle(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "rmsle_scorer = make_scorer(rmsle, greater_is_better=False)\n",
    "\n",
    "train   = pd.read_csv('train.csv', na_values=[''], keep_default_na=True)\n",
    "test    = pd.read_csv('test.csv',  na_values=[''], keep_default_na=True)\n",
    "test_ID = test['Id']\n",
    "\n",
    "y = np.log1p(train['SalePrice'])\n",
    "train.drop(columns='SalePrice', inplace=True)\n",
    "all_data = pd.concat([train, test], ignore_index=True)\n",
    "\n",
    "num_feats = all_data.select_dtypes(include=['int64','float64']).columns.drop('Id')\n",
    "skews     = all_data[num_feats].apply(lambda col: skew(col.dropna()))\n",
    "skewed    = skews[abs(skews) > 0.75].index\n",
    "all_data[skewed] = np.log1p(all_data[skewed])\n",
    "\n",
    "cat_feats = all_data.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "numeric_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler',  StandardScaler())\n",
    "])\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot',  OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_transformer,   num_feats),\n",
    "    ('cat', categorical_transformer, cat_feats)\n",
    "])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preproc', preprocessor),\n",
    "    ('reg',     ElasticNet(max_iter=5000, tol=1e-3))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'reg__alpha':    np.logspace(-3, 3, 7),\n",
    "    'reg__l1_ratio': [0.1, 0.5, 0.9]\n",
    "}\n",
    "\n",
    "search = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring=rmsle_scorer,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "X_train = all_data.iloc[:len(y)].drop(columns=['Id'])\n",
    "search.fit(X_train, y)\n",
    "print(\"Best parameters:\", search.best_params_)\n",
    "\n",
    "X_test    = all_data.iloc[len(y):].drop(columns=['Id'])\n",
    "preds_log = search.predict(X_test)\n",
    "preds     = np.expm1(preds_log)\n",
    "\n",
    "submission            = pd.read_csv('sample_submission.csv')\n",
    "submission['SalePrice'] = preds\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"Saved submission.csv\")"
   ],
   "id": "df9de7ccd1ecd12e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Подробности реализации модели ElasticNet на основе кода\n",
    "\n",
    "В данной реализации используется `ElasticNet` как базовая линейная модель с регуляризацией. Основное внимание в коде уделяется правильной настройке метрики качества, поиску гиперпараметров, а также точной обработке признаков.\n",
    "\n",
    "---\n",
    "\n",
    "### Настройка метрики RMSLE\n",
    "\n",
    "В качестве основной метрики используется **RMSLE** (корень из средней квадратичной логарифмической ошибки). Она задаётся вручную:\n",
    "\n",
    "```python\n",
    "def rmsle(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "rmsle_scorer = make_scorer(rmsle, greater_is_better=False)\n",
    "```\n",
    "\n",
    "Это кастомная метрика, основанная на mean_squared_error, но предполагающая, что логарифмирование целевой переменной уже произведено заранее. Поэтому log1p() применяется сразу к y перед обучением, а expm1() — после предсказания.\n",
    "\n",
    "Аргумент greater_is_better=False указывает GridSearchCV, что меньшие значения метрики — лучше.\n",
    "\n",
    "---\n",
    "\n",
    "### Выбор гиперпараметров\n",
    "\n",
    "Подбор гиперпараметров модели производится по двум ключевым параметрам:\n",
    "\n",
    "- `alpha` — коэффициент регуляризации: чем он больше, тем сильнее модель штрафует большие веса. Значения подбираются логарифмически:\n",
    "\n",
    "```python\n",
    "'reg__alpha': np.logspace(-3, 3, 7)  # от 0.001 до 1000\n",
    "```\n",
    "- `l1_ratio` — параметр смешивания регуляризаторов:\n",
    "$$\n",
    "\\rho \\in [0, 1]\n",
    "$$\n",
    "\n",
    "При $\\rho = 0$ используется только L2-регуляризация (Ridge),\n",
    "при $\\rho = 1$ — только L1 (Lasso),\n",
    "промежуточные значения соответствуют ElasticNet.\n",
    "\n",
    "В коде используется:\n",
    "\n",
    "```python\n",
    "'reg__l1_ratio': [0.1, 0.5, 0.9]\n",
    "```\n",
    "Это позволяет протестировать смещения в сторону Ridge, сбалансированного варианта и Lasso.\n",
    "\n",
    "---\n",
    "\n",
    "### GridSearchCV\n",
    "\n",
    "```python\n",
    "search = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring=rmsle_scorer,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "```\n",
    "- Используется 5-кратная кросс-валидация (`cv=5`), обеспечивающая устойчивую оценку модели на разных подвыборках.\n",
    "- `scoring=rmsle_scorer` — подключается ранее определённая метрика RMSLE.\n",
    "- `n_jobs=-1` — означает использование всех доступных процессоров для параллельной обработки.\n",
    "- `verbose=1` — включает вывод текущего хода выполнения на экран, полезно для отладки и мониторинга.\n",
    "\n",
    "---\n",
    "\n",
    "### Настройки модели ElasticNet\n",
    "\n",
    "```python\n",
    "ElasticNet(max_iter=5000, tol=1e-3)\n",
    "```\n",
    "- `max_iter=5000` — увеличенное количество итераций метода координатного спуска. Это важно при высокоразреженных данных, где обычного количества итераций может не хватить.\n",
    "- `tol=1e-3` — порог изменения функции потерь, при котором обучение прекращается. Позволяет избежать лишних итераций, если модель уже сходится.\n",
    "\n",
    "Такие параметры обеспечивают более стабильную и точную сходимость, особенно при использовании регуляризации.\n",
    "\n",
    "---\n",
    "\n",
    "### Обработка результатов\n",
    "\n",
    "После окончания обучения предсказания выполняются в логарифмической шкале:\n",
    "\n",
    "```python\n",
    "preds_log = search.predict(X_test)\n",
    "preds = np.expm1(preds_log)\n",
    "```\n",
    "- Модель обучалась на логарифмированных значениях `y`, поэтому предсказания (`preds_log`) также находятся в логарифмической шкале.\n",
    "- Функция `np.expm1(x)` выполняет обратное преобразование к `log1p(y)`, то есть вычисляет $e^x - 1$.\n",
    "- Использование `expm1` более численно устойчиво, особенно при малых значениях, чем `np.exp(x) - 1`.\n",
    "\n",
    "В результате восстанавливаются предсказанные значения в исходной шкале цен (`SalePrice`).\n",
    "\n",
    "---\n",
    "\n",
    "### Результат на Kaggle\n",
    "\n",
    "После выполнения обучения и предсказания, модель ElasticNet показала следующий результат на платформе Kaggle:\n",
    "\n",
    "> **Итоговая RMSLE: 0.12507**\n"
   ],
   "id": "ea9f7a227ab4ee41"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. CNN1D (одномерная сверточная сеть)\n",
    "\n",
    "Сверточные нейронные сети (CNN) — класс моделей, использующих локальные фильтры для извлечения признаков из входных данных. В задаче регрессии по табличным данным CNN может использоваться как способ автоматического построения нелинейных признаков с локальной структурой. Мы применяем одномерные свёртки (Conv1D), рассматривая табличные признаки как последовательность.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Формализация входа\n",
    "\n",
    "Пусть у нас есть обучающая выборка:\n",
    "\n",
    "$$\n",
    "D = \\{ (x^{(i)}, y^{(i)}) \\}_{i=1}^{m}\n",
    "$$\n",
    "\n",
    "где:\n",
    "\n",
    "- $x^{(i)}$ — вектор признаков (например, 79 штук),\n",
    "- $y^{(i)}$ — логарифмированная цена продажи дома,\n",
    "- $m$ — количество объектов в обучающей выборке.\n",
    "\n",
    "Вход в сверточную сеть — это $x^{(i)}$, рассматриваемый как одномерная последовательность длины $n$. Соответственно, данные имеют форму $(m, n, 1)$.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Свёртка (Conv1D)\n",
    "\n",
    "Пусть:\n",
    "\n",
    "- размер ядра (фильтра) равен \\( k \\),\n",
    "- веса ядра — \\( w = [w_1, w_2, ..., w_k] \\),\n",
    "- смещение — \\( b \\).\n",
    "\n",
    "Тогда результат применения фильтра на позиции \\( j \\) равен:\n",
    "\n",
    "$$\n",
    "z_j = \\sum_{i=1}^{k} w_i \\cdot x_{j+i-1} + b\n",
    "$$\n",
    "\n",
    "Если применить \\( F \\) фильтров, результат будет матрицей:\n",
    "\n",
    "$$\n",
    "Z \\in \\mathbb{R}^{(n - k + 1) \\times F}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Активация\n",
    "\n",
    "После свёртки применяется нелинейная функция, например ReLU:\n",
    "\n",
    "$$\n",
    "\\text{ReLU}(z) = \\max(0, z)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Выходные признаки\n",
    "\n",
    "Матрица \\( Z \\) содержит выходы всех фильтров по всем позициям:\n",
    "\n",
    "$$\n",
    "Z = [z^{(1)}, z^{(2)}, ..., z^{(F)}] \\in \\mathbb{R}^{(n - k + 1) \\times F}\n",
    "$$\n",
    "\n",
    "---\n",
    "### 5. Переход к регрессии\n",
    "\n",
    "- Применяется Flatten:\n",
    "\n",
    "$$\n",
    "h = \\text{Flatten}(Z) \\in \\mathbb{R}^d\n",
    "$$\n",
    "\n",
    "- Линейный выходной слой:\n",
    "\n",
    "$$\n",
    "\\hat{y} = w_d^\\top h + b\n",
    "$$\n",
    "- Или с двумя слоями и активациями:\n",
    "\n",
    "$$\n",
    "\\hat{y} = \\phi_2 \\left( W_2^T \\cdot \\phi_1 \\left( W_1^T \\cdot h + b_1 \\right) + b_2 \\right)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Функция потерь\n",
    "\n",
    "**MSE** (если логарифмирование применено заранее):\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{y}_i - y_i)^2\n",
    "$$\n",
    "\n",
    "**RMSLE** (если логарифмирование включено в метрику):\n",
    "\n",
    "$$\n",
    "\\text{RMSLE} = \\sqrt{ \\frac{1}{m} \\sum_{i=1}^{m} \\left( \\log(\\hat{y}_i + 1) - \\log(y_i + 1) \\right)^2 }\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Почему CNN1D применим к табличным данным\n",
    "\n",
    "- Признаки можно логически упорядочить,\n",
    "- Между соседними признаками могут быть связи,\n",
    "- Свёртки позволяют извлекать локальные шаблоны,\n",
    "- Параметры фильтров переиспользуются, что уменьшает переобучение."
   ],
   "id": "cd8e36a8d921f8e4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def load_data():\n",
    "    train = pd.read_csv('train.csv', na_values=[''], keep_default_na=True)\n",
    "    test  = pd.read_csv('test.csv',  na_values=[''], keep_default_na=True)\n",
    "\n",
    "    for c in ['Alley','PoolQC','Fence','MiscFeature']:\n",
    "        train.drop(columns=c, inplace=True)\n",
    "        test .drop(columns=c, inplace=True, errors='ignore')\n",
    "\n",
    "    for c in ['LotFrontage','MasVnrArea','GarageYrBlt']:\n",
    "        m = train[c].median()\n",
    "        train[c].fillna(m, inplace=True)\n",
    "        test [c].fillna(m, inplace=True)\n",
    "\n",
    "    num_cols = train.select_dtypes(include='number').columns.drop('SalePrice')\n",
    "    med      = train[num_cols].median()\n",
    "    train[num_cols].fillna(med, inplace=True)\n",
    "    test [num_cols].fillna(med, inplace=True)\n",
    "\n",
    "    cat_cols = train.select_dtypes(include='object').columns\n",
    "    for c in cat_cols:\n",
    "        train[c].fillna('None', inplace=True)\n",
    "        test [c].fillna('None', inplace=True)\n",
    "\n",
    "    ORD = {\n",
    "      'ExterQual':     [\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"],\n",
    "      'ExterCond':     [\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"],\n",
    "      'BsmtQual':      [\"None\",\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"],\n",
    "      'BsmtCond':      [\"None\",\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"],\n",
    "      'BsmtExposure':  [\"None\",\"No\",\"Mn\",\"Av\",\"Gd\"],\n",
    "      'BsmtFinType1':  [\"None\",\"Unf\",\"LwQ\",\"Rec\",\"BLQ\",\"ALQ\",\"GLQ\"],\n",
    "      'BsmtFinType2':  [\"None\",\"Unf\",\"LwQ\",\"Rec\",\"BLQ\",\"ALQ\",\"GLQ\"],\n",
    "      'HeatingQC':     [\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"],\n",
    "      'KitchenQual':   [\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"],\n",
    "      'FireplaceQu':   [\"None\",\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"],\n",
    "      'GarageQual':    [\"None\",\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"],\n",
    "      'GarageCond':    [\"None\",\"Po\",\"Fa\",\"TA\",\"Gd\",\"Ex\"],\n",
    "      'GarageFinish':  [\"None\",\"Unf\",\"RFn\",\"Fin\"],\n",
    "      'PavedDrive':    [\"N\",\"P\",\"Y\"]\n",
    "    }\n",
    "    for f, order in ORD.items():\n",
    "        train[f] = train[f].astype('category').cat.set_categories(order).cat.codes\n",
    "        if f in test:\n",
    "            test[f] = test[f].astype('category').cat.set_categories(order).cat.codes\n",
    "\n",
    "    rest = train.select_dtypes(include='object').columns\n",
    "    train = pd.get_dummies(train, columns=rest, drop_first=True)\n",
    "    test  = pd.get_dummies(test,  columns=rest, drop_first=True)\n",
    "\n",
    "    train, test = train.align(test, join='left', axis=1, fill_value=0)\n",
    "    test.drop(columns=['SalePrice'], inplace=True, errors='ignore')\n",
    "\n",
    "    y = np.log1p(train.pop('SalePrice'))\n",
    "\n",
    "    nums = train.select_dtypes(include='number').columns.drop('Id')\n",
    "    pt = PowerTransformer(method='yeo-johnson', standardize=False)\n",
    "    train[nums] = pt.fit_transform(train[nums])\n",
    "    test [nums] = pt.transform(test[nums])\n",
    "    scaler = StandardScaler()\n",
    "    train[nums] = scaler.fit_transform(train[nums])\n",
    "    test [nums] = scaler.transform(test[nums])\n",
    "\n",
    "    X      = train.drop(columns=['Id']).values.astype('float32')[..., None]\n",
    "    X_test = test .drop(columns=['Id']).values.astype('float32')[..., None]\n",
    "\n",
    "    return X, y.values.astype('float32'), X_test\n",
    "\n",
    "X, y, X_test = load_data()\n",
    "n_feat = X.shape[1]\n",
    "\n",
    "def build_model():\n",
    "    inp = layers.Input((n_feat, 1))\n",
    "\n",
    "    x = layers.Conv1D(64, 5, padding='same')(inp)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    x = layers.Conv1D(128, 3, padding='same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "\n",
    "    x = layers.Dense(256)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "\n",
    "    x = layers.Dense(64)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "\n",
    "    out = layers.Dense(1)(x)\n",
    "\n",
    "    model = models.Model(inp, out)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "kf  = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "oof = np.zeros_like(y)\n",
    "\n",
    "for fold, (tr, va) in enumerate(kf.split(X)):\n",
    "    X_tr, X_va = X[tr], X[va]\n",
    "    y_tr, y_va = y[tr], y[va]\n",
    "\n",
    "    tf.random.set_seed(100 + fold)\n",
    "    m = build_model()\n",
    "\n",
    "    es = callbacks.EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)\n",
    "    rl = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)\n",
    "\n",
    "    m.fit(\n",
    "        X_tr, y_tr,\n",
    "        validation_data=(X_va, y_va),\n",
    "        epochs=100,\n",
    "        batch_size=64,\n",
    "        callbacks=[es, rl],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    oof[va] = m.predict(X_va).flatten()\n",
    "\n",
    "print(\"OOF RMSLE:\", np.sqrt(mean_squared_error(y, oof)))\n",
    "\n",
    "tf.random.set_seed(999)\n",
    "final = build_model()\n",
    "es = callbacks.EarlyStopping(monitor='loss', patience=7, restore_best_weights=True)\n",
    "rl = callbacks.ReduceLROnPlateau(monitor='loss', factor=0.5, patience=5, verbose=1)\n",
    "\n",
    "final.fit(X, y, epochs=100, batch_size=64, callbacks=[es, rl], verbose=0)\n",
    "\n",
    "preds_log = final.predict(X_test).flatten()\n",
    "preds     = np.expm1(preds_log)\n",
    "\n",
    "sub = pd.read_csv('sample_submission.csv')\n",
    "sub['SalePrice'] = pd.to_numeric(preds, errors='coerce')\n",
    "median_price    = sub['SalePrice'].median()\n",
    "sub['SalePrice'].fillna(median_price, inplace=True)\n",
    "sub['SalePrice'] = sub['SalePrice'].astype(float)\n",
    "sub.to_csv('submission_cnn1d_best.csv', index=False)\n",
    "print(\"Saved submission_cnn1d_best.csv\")"
   ],
   "id": "85189847c32908ea"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Подробности реализации модели CNN1D на основе кода\n",
    "В данной реализации используется одномерная сверточная нейронная сеть (CNN1D), применённая к табличным данным. Модель реализована с использованием Keras (TensorFlow backend), с полной предобработкой и отладочной кросс-валидацией.\n",
    "___\n",
    "### Архитектура модели\n",
    "Модель строится в функции build_model() и включает следующие блоки:\n",
    "```python\n",
    "inp = layers.Input((n_feat, 1))\n",
    "```\n",
    "- **Вход**: одномерный тензор размера `(n_feat, 1)` — каждый признак интерпретируется как временная точка.\n",
    "\n",
    "**Свёрточный блок 1:**\n",
    "```python\n",
    "x = layers.Conv1D(64, 5, padding='same')(inp)\n",
    "```\n",
    "- **64 фильтра**, каждый шириной 5, шаг по умолчанию — 1;\n",
    "- padding='same' сохраняет длину выхода.\n",
    "```python\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Activation('relu')(x)\n",
    "x = layers.MaxPooling1D(2)(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "```\n",
    "- **BatchNorm** стабилизирует обучение;\n",
    "- **ReLU** — нелинейная активация;\n",
    "- **MaxPooling1D(2)** уменьшает размерность вдвое;\n",
    "- **Dropout(0.3)** — регуляризация, отключает 30% нейронов.\n",
    "\n",
    "**Свёрточный блок 2:**\n",
    "```python\n",
    "x = layers.Conv1D(128, 3, padding='same')(x)\n",
    "```\n",
    "- **128 фильтров**, ширина фильтра — 3.\n",
    "```python\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Activation('relu')(x)\n",
    "x = layers.MaxPooling1D(2)(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "```\n",
    "— те же операции, но с другим числом фильтров и уменьшенной шириной.\n",
    "\n",
    "**Переход к dense-части:**\n",
    "```python\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "```\n",
    "- **Глобальный пулинг** — берёт среднее по всей временной оси для каждого фильтра, сильно уменьшает размерность и повышает устойчивость.\n",
    "___\n",
    "\n",
    "### Полносвязные слои\n",
    "```python\n",
    "x = layers.Dense(256)(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Activation('relu')(x)\n",
    "x = layers.Dropout(0.4)(x)\n",
    "```\n",
    "- Полносвязный слой на 256 нейронов;\n",
    "- Dropout (0.4) — сильная регуляризация (40%).\n",
    "```python\n",
    "x = layers.Dense(64)(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Activation('relu')(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "```\n",
    "- Ещё один Dense (64 нейрона) + регуляризация.\n",
    "```python\n",
    "out = layers.Dense(1)(x)\n",
    "```\n",
    "- Один выходной нейрон — регрессионный выход (логарифм цены дома).\n",
    "___\n",
    "### Компиляция модели\n",
    "```python\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "```\n",
    "- **Adam** — адаптивный оптимизатор (изменяет шаг обучения для каждого веса);\n",
    "- **MSE** — среднеквадратичная ошибка на логарифмах цен.\n",
    "___\n",
    "### Стратегия обучения\n",
    "#### Используется 5-кратная кросс-валидация (KFold):\n",
    "```python\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "```\n",
    "- shuffle=True + seed — для стабильности;\n",
    "- обучаем модель на 4 фолдах, валидируем на 5-м.\n",
    "\n",
    "#### EarlyStopping\n",
    "```python\n",
    "callbacks.EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)\n",
    "```\n",
    "- Останавливает обучение, если val_loss не улучшается 7 эпох подряд;\n",
    "- restore_best_weights=True — возвращает модель к лучшему состоянию.\n",
    "\n",
    "#### ReduceLROnPlateau\n",
    "```python\n",
    "callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5)\n",
    "```\n",
    "- Если val_loss не улучшается 5 эпох, learning rate уменьшается вдвое;\n",
    "- Способ адаптивной регулировки скорости обучения.\n",
    "___\n",
    "### Обучение\n",
    "```python\n",
    "m.fit(..., epochs=100, batch_size=64, ...)\n",
    "```\n",
    "- Модель обучается максимум 100 эпох;\n",
    "- Размер батча: 64;\n",
    "- Реально обучение завершается раньше благодаря EarlyStopping.\n",
    "___\n",
    "### Финальное предсказание\n",
    "После валидации на 5 фолдах, модель переобучается на всех данных (X, y), предсказывает X_test.\n",
    "```python\n",
    "preds_log = final.predict(X_test).flatten()\n",
    "preds = np.expm1(preds_log)\n",
    "```\n",
    "- Предсказания в лог-шкале → восстанавливаем реальные цены через expm1.\n",
    "___\n",
    "### Результат на Kaggle\n",
    "\n",
    "После выполнения обучения и предсказания, модель ElasticNet показала следующий результат на платформе Kaggle:\n",
    "\n",
    "> **Итоговая RMSLE: 0.56126**"
   ],
   "id": "500c6c9a7a028c85"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## MLP (полносвязная нейронная сеть)\n",
    "MLP (Multi-Layer Perceptron) — классическая нейронная сеть, в которой каждый нейрон на слое связан со всеми нейронами предыдущего слоя. В контексте регрессии по табличным данным MLP обучается преобразовывать вектор признаков в непрерывное значение — прогноз логарифма цены.\n",
    "___\n",
    "### 1. Формализация входа\n",
    "Пусть обучающая выборка имеет вид:\n",
    "\n",
    "$$\n",
    "D = { (x^{(i)}, y^{(i)}) }_{i=1}^{m}\n",
    "$$\n",
    "\n",
    "где:\\\n",
    "\t•\t$x^{(i)} \\in \\mathbb{R}^n$ — вектор признаков,\\\n",
    "\t•\t$y^{(i)} \\in \\mathbb{R}$ — логарифм цены (целевое значение),\\\n",
    "\t•\t$m$ — число объектов.\n",
    "\n",
    "Модель принимает входной вектор x и последовательно преобразует его с помощью нескольких полносвязных слоёв и активаций.\n",
    "___\n",
    "### 2. Архитектура сети\n",
    "Общая схема MLP выглядит так:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "h^{(1)} &= \\phi_1(W^{(1)} x + b^{(1)}) \\\\\n",
    "h^{(2)} &= \\phi_2(W^{(2)} h^{(1)} + b^{(2)}) \\\\\n",
    "&\\vdots \\\\\n",
    "h^{(L-1)} &= \\phi_{L-1}(W^{(L-1)} h^{(L-2)} + b^{(L-1)}) \\\\\n",
    "\\hat{y} &= W^{(L)} h^{(L-1)} + b^{(L)}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "где:\\\n",
    "\t•\t$W^{(l)}$ — матрица весов слоя $l$,\\\n",
    "\t•\t$b^{(l)}$ — вектор смещений,\\\n",
    "\t•\t$\\phi_l$ — функция активации (обычно ReLU),\\\n",
    "\t•\t$L$ — количество слоёв (включая выходной),\\\n",
    "\t•\t$h^{(l)}$ — активации на слое $l$,\\\n",
    "\t•\t$\\hat{y}$ — предсказание (лог-цена).\n",
    "___\n",
    "### 3. Активации\n",
    "На скрытых слоях используются нелинейные функции:\n",
    "\n",
    "$$\n",
    "\\phi(x) = \\max(0, x) \\quad \\text{(ReLU)}\n",
    "$$\n",
    "\n",
    "Эта функция помогает сети моделировать нелинейные зависимости между признаками.\n",
    "___\n",
    "### 4. Регуляризация и нормализация\n",
    "Чтобы повысить устойчивость модели, применяются:\n",
    "- Dropout — случайное зануление нейронов:\n",
    "Например, Dropout(0.4) означает, что 40% выходов слоя обнуляются случайно при обучении.\n",
    "- Batch Normalization — нормализация выходов слоя:\n",
    "$$\n",
    "\\text{BN}(x) = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\varepsilon}}\n",
    "$$\n",
    "где $\\mu$ и $\\sigma^2$ — среднее и дисперсия по батчу, $\\varepsilon$ — маленькая константа для численной устойчивости.\n",
    "\n",
    "Это ускоряет обучение и делает модель менее чувствительной к выбору параметров.\n",
    "___\n",
    "### 5. Выходной слой\n",
    "Выходной слой состоит из одного нейрона без активации:\n",
    "\n",
    "$$\n",
    "\\hat{y} = w^\\top h^{(L-1)} + b\n",
    "$$\n",
    "\n",
    "где $h^{(L-1)}$ — последняя скрытая репрезентация, $w$ и $b$ — параметры.\n",
    "___\n",
    "### 6. Функция потерь\n",
    "Для обучения используется среднеквадратичная ошибка (MSE):\n",
    "\n",
    "$$\n",
    "\\text{MSE} = \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{y}_i - y_i)^2\n",
    "$$\n",
    "\n",
    "Если логарифмирование целевой переменной уже применено (как в нашем случае), RMSLE не используется, и MSE работает как прямая метрика.\n",
    "___\n",
    "### 7. Почему MLP применим к табличным данным\n",
    "- Способен моделировать как линейные, так и сложные нелинейные зависимости,\n",
    "- Не требует упорядоченности признаков,\n",
    "- Легко масштабируется на большие таблицы,\n",
    "- Работает «в лоб» — просто принимает вектор признаков и выдаёт число."
   ],
   "id": "4b511a2902bf2c7b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import skew\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def rmsle(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "rmsle_scorer = make_scorer(rmsle, greater_is_better=False)\n",
    "\n",
    "train = pd.read_csv('train.csv', na_values=[''], keep_default_na=True)\n",
    "test  = pd.read_csv('test.csv',  na_values=[''], keep_default_na=True)\n",
    "test_ID = test['Id']\n",
    "y = np.log1p(train['SalePrice'])\n",
    "train.drop(columns=['SalePrice'], inplace=True)\n",
    "data = pd.concat([train, test], ignore_index=True)\n",
    "\n",
    "num_feats = data.select_dtypes(['int64','float64']).columns.drop('Id')\n",
    "skews = data[num_feats].apply(lambda x: skew(x.dropna()))\n",
    "skewed = skews[abs(skews) > 0.75].index\n",
    "data[skewed] = np.log1p(data[skewed])\n",
    "\n",
    "cat_feats = data.select_dtypes(['object']).columns.tolist()\n",
    "\n",
    "num_pipe = Pipeline([\n",
    "    ('impute', SimpleImputer(strategy='median')),\n",
    "    ('scale',  StandardScaler())\n",
    "])\n",
    "cat_pipe = Pipeline([\n",
    "    ('impute', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ohe',    OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preproc = ColumnTransformer([\n",
    "    ('num', num_pipe, num_feats),\n",
    "    ('cat', cat_pipe, cat_feats)\n",
    "])\n",
    "\n",
    "mlp = Pipeline([\n",
    "    ('prep', preproc),\n",
    "    ('model', MLPRegressor(hidden_layer_sizes=(512,256,128),\n",
    "                           activation='relu',\n",
    "                           solver='adam',\n",
    "                           alpha=1e-4,\n",
    "                           learning_rate_init=1e-3,\n",
    "                           max_iter=200,\n",
    "                           random_state=42))\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'model__alpha': [1e-4, 1e-3, 1e-2],\n",
    "    'model__learning_rate_init': [1e-3, 1e-2]\n",
    "}\n",
    "search = GridSearchCV(mlp, param_grid, cv=5, scoring=rmsle_scorer, n_jobs=-1, verbose=1)\n",
    "X_train = data.iloc[:len(y)].drop(columns=['Id'])\n",
    "search.fit(X_train, y)\n",
    "print(\"MLP best params:\", search.best_params_)\n",
    "\n",
    "X_test = data.iloc[len(y):].drop(columns=['Id'])\n",
    "preds_log = search.predict(X_test)\n",
    "preds = np.expm1(preds_log)\n",
    "\n",
    "sub = pd.read_csv('sample_submission.csv')\n",
    "sub['SalePrice'] = preds\n",
    "sub.to_csv('submission_mlp.csv', index=False)\n",
    "print(\"Saved submission_mlp.csv\")"
   ],
   "id": "3d3834f3dd37783a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Подробности реализации модели MLP на основе кода\n",
    "В данной реализации используется классическая полносвязная нейронная сеть (MLP) из sklearn.neural_network.MLPRegressor, встроенная в Pipeline с полной предобработкой данных и автоматическим подбором гиперпараметров.\n",
    "___\n",
    "### Предобработка данных\n",
    "Масштабирование и кодирование признаков выполняется с помощью ColumnTransformer:\n",
    "```python\n",
    "preproc = ColumnTransformer([\n",
    "    ('num', num_pipe, num_feats),\n",
    "    ('cat', cat_pipe, cat_feats)\n",
    "])\n",
    "```\n",
    "- Числовые признаки:\n",
    "    - Заполняются медианой (SimpleImputer(strategy='median'))\n",
    "    - Масштабируются (StandardScaler)\n",
    "- Категориальные признаки:\n",
    "    - Заполняются наиболее частым значением (SimpleImputer(strategy='most_frequent'))\n",
    "    - Кодируются с помощью OneHotEncoder (игнорируя неизвестные значения)\n",
    "\n",
    "Перед этим:\n",
    "- Числовые признаки со скошенным распределением логарифмируются с помощью np.log1p(), если $|\\text{skew}| > 0.75$.\n",
    "___\n",
    "### Архитектура модели\n",
    "```python\n",
    "MLPRegressor(\n",
    "    hidden_layer_sizes=(512, 256, 128),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=1e-4,\n",
    "    learning_rate_init=1e-3,\n",
    "    max_iter=200,\n",
    "    random_state=42\n",
    ")\n",
    "```\n",
    "- **Слои:** три скрытых слоя с 512, 256 и 128 нейронами.\n",
    "- **Активация:** ReLU — стандартная нелинейность $\\phi(x) = \\max(0, x)$.\n",
    "- **Оптимизатор:** adam — адаптивный стохастический градиент.\n",
    "- **Регуляризация:** alpha=1e-4 — L2-регуляризация (аналог weight decay).\n",
    "- **Начальный learning rate:** 1e-3 — базовая скорость обучения.\n",
    "- **Максимум итераций:** 200 эпох (может не сойтись, если данные сложные).\n",
    "- **random_state=42** — для воспроизводимости результата.\n",
    "___\n",
    "### Подбор гиперпараметров\n",
    "\n",
    "Подбор осуществляется с помощью GridSearchCV по сетке:\n",
    "```python\n",
    "param_grid = {\n",
    "    'model__alpha': [1e-4, 1e-3, 1e-2],\n",
    "    'model__learning_rate_init': [1e-3, 1e-2]\n",
    "}\n",
    "```\n",
    "- Подбираются два параметра:\n",
    "    - alpha — коэффициент регуляризации;\n",
    "    - learning_rate_init — начальная скорость обучения.\n",
    "- Используется 5-кратная кросс-валидация (cv=5);\n",
    "- Метрика: rmsle_scorer — кастомная RMSLE-функция (при логарифмированной y):\n",
    "```python\n",
    "def rmsle(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "```\n",
    "___\n",
    "### Сабмит и восстановление цен\n",
    "После получения логарифмированных предсказаний:\n",
    "```python\n",
    "preds_log = search.predict(X_test)\n",
    "preds = np.expm1(preds_log)\n",
    "```\n",
    "- Используется np.expm1 — точное восстановление из log1p;\n",
    "- Создаётся файл submission_mlp.csv в требуемом формате.\n",
    "\n",
    "___\n",
    "### Результат на Kaggle\n",
    "Несмотря на простоту архитектуры и ограниченные возможности настройки через MLPRegressor, модель достигла на Kaggle результата:\n",
    "\n",
    "> **Итоговая RMSLE: 0.15466**\n",
    "\n",
    "Для модели без ручной настройки глубокой архитектуры и встроенного early stopping — это вполне достойный результат. Улучшение возможно при переходе к фреймворкам с полной поддержкой обучения нейросетей (Keras, PyTorch)."
   ],
   "id": "c09622fcda862aca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Вывод\n",
    "После обучения и тестирования трёх различных архитектур были получены следующие значения метрики **RMSLE** на Kaggle:\n",
    "\n",
    "| Модель                  | RMSLE       |\n",
    "|------------------------|-------------|\n",
    "| **ElasticNet**         | **0.12507** |\n",
    "| **MLP (полносвязная)** | 0.15466     |\n",
    "| **CNN1D (свёрточная)** | 0.56126     |\n",
    "\n",
    "---\n",
    "- **ElasticNet** показал **наилучший результат**, несмотря на простую линейную природу. Это объясняется тем, что:\n",
    "  - данные хорошо масштабированы и подготовлены;\n",
    "  - логарифмирование целевой переменной снизило влияние выбросов;\n",
    "  - регуляризация L1/L2 позволила избежать переобучения;\n",
    "  - модель хорошо справляется с табличными данными, где признаки независимы.\n",
    "\n",
    "- **MLP** (нейронная сеть с 3 слоями) занял **второе место**. Он смог уловить более сложные зависимости, чем линейная модель, но:\n",
    "  - требует точной настройки гиперпараметров;\n",
    "  - чувствителен к переобучению;\n",
    "  - хуже интерпретируем.\n",
    "\n",
    "- **CNN1D** показал себя **наихудше** на табличных данных:\n",
    "  - несмотря на глубокую архитектуру, модель не смогла эффективно использовать локальные зависимости;\n",
    "  - признаки в таблицах не обладают пространственной структурой, необходимой для свёрток;\n",
    "  - архитектура CNN требует существенно другой подход к представлению данных (например, графовые или временные зависимости).\n",
    "\n",
    "---\n",
    "Это задание стало моим первым практическим знакомством с нейросетевыми архитектурами. Процесс оказался местами сложным и не всегда понятным: особенно в части настройки параметров и понимания математики, лежащей в основе моделей. Для повышения качества в будущем мне необходимо глубже разобраться:\n",
    "- в математической сути нейронных сетей;\n",
    "- в принципах регуляризации и нормализации;\n",
    "- в подходах к поиску оптимальных архитектур и гиперпараметров.\n",
    "\n",
    "Тем не менее, этот проект дал мне базовое понимание различий между классическими и нейросетевыми методами, а также важности предобработки при работе с табличными данными.\n"
   ],
   "id": "3a37003fef8a1334"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
